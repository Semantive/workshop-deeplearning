{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Sieci neuronowe w TensorFlow\n",
    "*** \n",
    "Ćwiczenia w tym Notebooku dotyczą już tylko sieci neuronowych. Na początek zobaczysz jak\n",
    "tworzy się sieci neuronowe w TensorFlow.\n",
    "Następnie utworzysz zestaw funkcji, które posłużą Ci do stworzenia głębokiej sieci\n",
    "neuronowej, do rozpoznawania cyfr pisma ręcznego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sieć neuronowa z jedną warstwą ukrytą\n",
    "\n",
    "Do tego modelu ponownie użyjemy zbioru z poprzedniego Notebooka, lecz tym razem będziemy rozpoznawać wszystkie \n",
    "10 cyfr dzięki funkcji SOFTMAX w warstwie wyjściowej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import danych\n",
    "X,y = load_digits(n_class=10, return_X_y=True)\n",
    "\n",
    "np.random.seed(54321)\n",
    "p = np.random.permutation(len(y))\n",
    "X = X[p]\n",
    "y = y[p].reshape(-1,1)\n",
    "\n",
    "test_set_size = round(len(y) * 0.25)\n",
    "X_train = X[:-test_set_size]\n",
    "y_train = y[:-test_set_size]\n",
    "\n",
    "X_test = X[-test_set_size:]\n",
    "y_test = y[-test_set_size:]\n",
    "\n",
    "print(\"Wymiary X:\", X_train.shape)\n",
    "print(\"Wymiary y:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ćwiczenie 1.1\n",
    "1. Ustaw hiperparametry modelu - liczbę neurnonów w warstwie ukrytej na 100, learning_rate na 0.01, liczbe epok na 3000.\n",
    "2. Zdefiniuj placeholdery: X_input:typ - tf.float32 o odpowiednim kształcie i name = 'x_input'; y_target: typ tf.int32, o odpowiednim kształcie i name = 'y_target'.\n",
    "3. Oblicz z1 wg wzoru $Z_1 = X * W_1 + b_1$ a następnie zaaplikuj sigmoidalną funkcję aktywacji (tf.nn.sigmoid()).\n",
    "4. Zainicjalizuj W2 i b2, nazwy weight_2 i bias_2, odpowiednio dobierając wymiary, wzoruj się na poprzedniej warstwie.\n",
    "5. Oblicz z2 wg wzoru $Z_2 = A_1 * W_2 + b_2$\n",
    "5. W sesji, w kolejnej komórce wywołaj optimizer i dostarcz do grafu odpowiednie dane (feed_dict).\n",
    "6. Wytrenuj sieć.\n",
    "7. Zobacz jak zmiana hiperparametrów wpływa na jej rezultaty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#hiperparametry modelu\n",
    "hidden_layer_neurons =###Miejsce na Twój kod\n",
    "learning_rate =###Miejsce na Twój kod\n",
    "n_epochs =###Miejsce na Twój kod\n",
    "\n",
    "#wymiary zmiennych wejściowych\n",
    "n_outputs = len(np.unique(y_train))\n",
    "n_inputs = X_train.shape[1]\n",
    "\n",
    "X_input = ###Miejsce na Twój kod\n",
    "y_target = ###Miejsce na Twój kod\n",
    "#kodowanie one hot zmiennej celu\n",
    "y_onehot = tf.reshape(tf.one_hot(y_target, depth=n_outputs, axis=1), (-1,n_outputs))\n",
    "\n",
    "#Pierwsza warstwa\n",
    "initialization_1 = tf.truncated_normal((n_inputs, hidden_layer_neurons), stddev=0.01, seed=54321)\n",
    "W1 = tf.get_variable(name=\"weights_1\", initializer=initialization_1)\n",
    "b1 = tf.get_variable(name='bias_1', initializer=tf.zeros([hidden_layer_neurons]))\n",
    "z1 = ###Miejsce na Twój kod\n",
    "a1 = ###Miejsce na Twój kod\n",
    "\n",
    "\n",
    "#Druga warstwa - wyjściowa\n",
    "initialization_2 = tf.truncated_normal((###Miejsce na Twój kod), stddev=0.01, seed=54321)\n",
    "W2 = ###Miejsce na Twój kod\n",
    "b2 = ###Miejsce na Twój kod\n",
    "z2 = ###Miejsce na Twój kod\n",
    "y_predictions = tf.nn.softmax(z2)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=z2, labels=y_onehot))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_onehot, 1), tf.argmax(y_predictions,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        ###Miejsce na Twój kod\n",
    "        if epoch % 200 == 0:\n",
    "            epoch_loss = sess.run(loss, feed_dict={X_input: X_train, y_target: y_train})\n",
    "            epoch_accuracy = sess.run(accuracy,feed_dict={X_input: X_train, y_target: y_train})\n",
    "            print(\"Epoch:{}, loss:{}, accuracy:{}\".format(epoch, epoch_loss, epoch_accuracy))\n",
    "    \n",
    "    print(\"\\nFINAL METRICS:\")\n",
    "    print(\"Training set accuracy: \", accuracy.eval({X_input: X_train, y_target: y_train}))\n",
    "    print(\"Test set accuracy: \", accuracy.eval({X_input:X_test, y_target: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dodatkowe elementy  sieci neuronowej\n",
    "***\n",
    "Od tego ćwiczenia zaczynamy tworzyć pomocnicze funkcje do modelu głębokiej sieci neuronowej.\n",
    "W tym zadaniu uzupełnianie kodu będzie wiązało się z większą samodzielnością, warto zatem przy tworzeniu modelu \n",
    "podpatrywać rozwiązania z poprzednich przykładów.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Różne funkcje aktywacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ćwiczenie 2.1\n",
    "Tworzymy funkcje pomocniczą dodającą warstwę do modelu sieci neuronowej.\n",
    "1. Zainicjalizuj zmienne W i b, oraz oblicz z, zwróć szególną uwagę na wymiary, wzoruj się na poprzednim przykładzie\n",
    "2. Nałóż na warstwę odpowiednia funkcję aktywacji w zależności od parametru 'activation' (tf.nn.sigmoid(), tf.nn.relu())\n",
    "3. Zwróć obiekt tuple w którym pierwszym elementem jest warstwa z nałożona funkcją aktywacje a drugim wagi W danej warstwy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_layer(X, n_units, name, activation='relu'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Funkcja tworząca pojedynczą warstwę\n",
    "    :param X: Dane uczące, o wymiarach liczba przypadków x liczba zmiennych\n",
    "    :type x: Tensor\n",
    "    :param n_units: Liczba neuronów w sieci\n",
    "    :type n_units: int\n",
    "    :param name: Nazwa warstwy\n",
    "    :type name: string\n",
    "    :param activations: Funkcja aktywacji - relu badź sigmoid\n",
    "    :type activation: string\n",
    "    :returns: (wartość wyjściowa warstwy, wektor wag)\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        initialization = tf.truncated_normal((n_inputs, n_units), stddev=0.01, seed=54321)\n",
    "        W = ###Miejsce na Twój kod\n",
    "        b = ###Miejsce na Twój kod\n",
    "        z = ###Miejsce na Twój kod\n",
    "        if activation == 'relu':\n",
    "            return ###Miejsce na Twój kod\n",
    "        elif activation == 'sigmoid':\n",
    "            return ###Miejsce na Twój kod\n",
    "        else:\n",
    "            return (z, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.2 Regularyzacja L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ćwiczenie 2.2\n",
    "1. Użyj funkcji  tf.nn.l2_loss() listy wag - weights, a wyniki zsumuj w tensorze l2_regularizer\n",
    "2. Utwórz funkcje zwaracającą funkcje straty z nałożoną regularyzacją L2. Działaj wg uproszczonego wzoru $1/m(\\sum (loss + \\lambda * l2regularizer))$   wykorzystaj tf.reduce_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_l2_regularization(loss, weights, lambd):\n",
    "    '''\n",
    "    Funkcja do wliczenia regularyzacji w funkcję kosztu\n",
    "    \n",
    "    :param loss: funkcja straty\n",
    "    :type loss: tensor\n",
    "    :param weights: suma wektor wszystkich wag w modelu\n",
    "    :type weights: tensor\n",
    "    :param lambd: parametr lambda dla regularyzacji l2\n",
    "    :type lambd: float\n",
    "    :returns: funkcje kosztu z dodaną regularyzacją\n",
    "    '''\n",
    "    l2_regularizer = tf.zeros(shape=())\n",
    "    for i in range(len(weights)):\n",
    "        l2_regularizer += ###Miejsce na Twój kod\n",
    "    return ###Miejsce na Twój kod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdź poprawność funkcji z pomocą dwóch poniższych komórek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(dtype=tf.float32, shape=(None, 2), name=\"input\")\n",
    "y = tf.placeholder(dtype=tf.float32, shape=(None, 1), name=\"target\")\n",
    "\n",
    "W = tf.get_variable(\"Weight\", dtype=tf.float32, initializer=tf.random_normal((2,1),seed=321))\n",
    "b = tf.get_variable(\"Bias\", initializer=tf.zeros((1,1)))\n",
    "\n",
    "#Model\n",
    "model = tf.matmul(X, W) + b\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model,labels=y))\n",
    "regularized_loss = add_l2_regularization(loss, W, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(regularized_loss, feed_dict={X: [[5,5],[4,5],[1,1],[3,2]], y:[[1],[1],[0],[0]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wynik: 0.19133858"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_layer(X, keep_prob=1.0):\n",
    "    \"\"\"\n",
    "    Dodanie dropout do danej warstwy sieci\n",
    "    :param X: dane wejściowe bądź warstwa sieci\n",
    "    :type X: tensor\n",
    "    :param keep_prob: Prawdopodobieństwo dla każdego neuronu, że nie zostanie wyłączony\n",
    "    :type keep_prob: float\n",
    "    :returns: warstwa sieci z nałóżonym dropoutem\n",
    "    \"\"\"\n",
    "    return tf.nn.dropout(X, keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Głęboka sieć neuronowa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ćwiczenie 3.1 \n",
    "Wykorzystaj utworzone wcześniej funkcje do stworzenia funkcji z modelem sieci neuronowej z 3 warstawmi ukrytymi, pamiętaj o odpowiednich argumentach funkcji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(X, y, n_outputs, input_size=64,\n",
    "               hidden_layer_1_neurons=100,\n",
    "               hidden_layer_2_neurons=50, \n",
    "               hidden_layer_3_neurons=50,\n",
    "               dropout_keep_prob = 1.0,\n",
    "               activation='relu'):\n",
    "     \"\"\"\n",
    "    Funkcja tworzaca sieć neuronową z trzema warstwami ukrytymi\n",
    "    \n",
    "    :param X: dane wejściowe\n",
    "    :type X: tensor\n",
    "    :param y: zmienna celu\n",
    "    :type y: tensor\n",
    "    :pram n_outputs: liczba klas zmiennej celu\n",
    "    :type n_outputs: int\n",
    "    :param input_size: liczba zmiennych wejściowych\n",
    "    :type input_size: int\n",
    "    :param hidden_layer_1_neurons: liczba neurnów w 1. warstwie ukrytej\n",
    "    :type hidden_layer_1_neurons: int\n",
    "    :param hidden_layer_2_neurons: liczba neurnów w 2. warstwie ukrytej\n",
    "    :type hidden_layer_2_neurons: int\n",
    "    :param hidden_layer_3_neurons: liczba neurnów w 3. warstwie ukrytej\n",
    "    :type hidden_layer_3_neurons: int\n",
    "    :param dropout_keep_prob: Prawdopodobieństwo dla każdego neuronu, że nie zostanie wyłączony\n",
    "    :param activations: Funkcja aktywacji - relu badź sigmoid\n",
    "    :type activation: string\n",
    "    :returns:y_predictions - predykcje, loss- funkcje straty, W- lista ze wszystkimi wagami \n",
    "    \"\"\"\n",
    "    \n",
    "    hidden_layer_1, W1 = ###Miejsce na Twój kod\n",
    "    dropout_1 = ###Miejsce na Twój kod\n",
    "    hidden_layer_2, W2 = ###Miejsce na Twój kod\n",
    "    dropout_2 = ###Miejsce na Twój kod\n",
    "    hidden_layer_3, W3 = ###Miejsce na Twój kod\n",
    "    \n",
    "    logits, W4 = ann_layer(hidden_layer_3, n_outputs, name=\"output\", activation=None)\n",
    "\n",
    "    y_predictions = tf.nn.softmax(logits)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y))\n",
    "    W = [W1,W2,W3,W4]\n",
    "    return y_predictions, loss, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ćwiczenie 3.2 \n",
    "Analogicznie do algorytmu spadku gradientu zaimplementuj algortym Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(loss, learning_rate=0.001, optimizer='GD'):\n",
    "    \"\"\"\n",
    "    Funkcja zwracająca odpowiedni optymalizator\n",
    "    :param loss: funkcja straty\n",
    "    :type loss: tensor\n",
    "    :param learning_rate: hiperparametr wielkości kroku przy optymalizacji\n",
    "    :type learning_rate: float\n",
    "    :param optimizer: wybrany optymalizator - \"GD\" - Gradient Descent, \"Adam\" - Adam\n",
    "    :type optimizer: string\n",
    "    \"\"\"\n",
    "    if optimizer == 'GD':\n",
    "        return tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    elif optimizer == \"Adam\":\n",
    "        return ###Miejsce na Twój kod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y, y_predictions):\n",
    "    \"\"\"\n",
    "    Obliczanie dokładności predykcji\n",
    "    \n",
    "    :param y: rzeczywiste wartości zmiennej celu\n",
    "    :type y: tensor\n",
    "    :param y_predictions: wyestymowane wartości zmiennej celu\n",
    "    :type y: tensor\n",
    "    \"\"\"\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_predictions,1))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ćwiczenie 3.3\n",
    "Korzystając z wcześniej utworzonych funkcji zaimplementuj funkcję do trenowania i ewaluacji modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, y_train, X_test, y_test,\n",
    "          hidden_layer_1_neurons=100,\n",
    "          hidden_layer_2_neurons=50,\n",
    "          hidden_layer_3_neurons=50,\n",
    "          learning_rate=0.07,\n",
    "          activation='relu',\n",
    "          n_epochs=1000,\n",
    "          l2_regularization=True,\n",
    "          l2_lambda=0.1,\n",
    "          dropout_keep_prob=1.0,\n",
    "          optimizer='GD',\n",
    "          plot_losses=True):\n",
    "    \"\"\"\n",
    "    Tworzenie, trening i ewaluacja modelu\n",
    "    \n",
    "    :param X_train: tablica zmiennych wejściowych dla danych treninigowych\n",
    "    :type: X_train: numpy array\n",
    "    :param y_train: wektor zmiennej celu dla danych treningowych\n",
    "    :type y_train: numpu array\n",
    "    :param X_test: tablica zmiennych wejściowych dla danych testowych\n",
    "    :type: X_test: numpy array\n",
    "    :param y_test: wektor zmiennej celu dla danych testowych\n",
    "    :type y_test: numpu array\n",
    "    :param learning_rate: hiperparametr wielkości kroku przy optymalizacji\n",
    "    :type learning_rate: float\n",
    "    :param activations: Funkcja aktywacji - relu badź sigmoid\n",
    "    :type activation: string\n",
    "    :param n_epochs: liczba epok treningowych\n",
    "    :type n_epochs: int\n",
    "    :param l2_regularization: czy używać regularyzacji l2\n",
    "    :type l2_regularization: boolean\n",
    "    :param l2_lambda: parametr lambda w regularyzacji l2\n",
    "    :type l2_lambda: float\n",
    "    :param dropout_keep_prob: Prawdopodobieństwo dla każdego neuronu, że nie zostanie wyłączony\n",
    "    :type dropout_keep_prob: float\n",
    "    :param optimizer: wybrany optymalizator - \"GD\" - Gradient Descent, \"Adam\" - Adam\n",
    "    :type optimizer: string\n",
    "    :param plot_losses: czy rysować wykres funkcji straty\n",
    "    :type plot_losses: boolean\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    losses = []\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    n_outputs = len(np.unique(y_train))\n",
    "    n_inputs = X_train.shape[1]\n",
    "    \n",
    "    X_input = ##Miejsce na placeholder typ tf.float32\n",
    "    y_target = ##Miejsce na placeholder typ tf.int32 \n",
    "    keep_prob = ##Miejsce na placeholder typ tf.float32, shape=()\n",
    "    \n",
    "    y_onehot = tf.reshape(tf.one_hot(y_target, depth=n_outputs, axis=1), (-1, n_outputs))\n",
    "    y_predictions, loss, W = ## Miejsce na model sieci neuronowej o odpowiednich parametrach\n",
    "    \n",
    "    if l2_regularization:\n",
    "        loss = ##Miejsce na regularyzację\n",
    "    \n",
    "    optimizer = ##Miejsce na optymalizator\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    accuracy = compute_accuracy(y_onehot, y_predictions)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(n_epochs):\n",
    "            ###Miejsce na trening modelu / keep_prob=dropout_keep_prob\n",
    "            if  epoch % 50 == 0:\n",
    "                epoch_loss = sess.run(loss, feed_dict={X_input: X_train, y_target: y_train, keep_prob: 1.0})\n",
    "                epoch_accuracy = sess.run(accuracy,feed_dict={X_input: X_train, y_target: y_train, keep_prob: 1.0})\n",
    "                print(\"Epoch:{}, loss:{}, accuracy:{}\".format(epoch, epoch_loss, epoch_accuracy))\n",
    "                losses.append(epoch_loss)\n",
    "                \n",
    "        if plot_losses:\n",
    "            plt.plot(losses)\n",
    "            plt.show()\n",
    "        \n",
    "        print(\"Training set accuracy: \", accuracy.eval({X_input: X_train, y_target: y_train, keep_prob: 1.0}))\n",
    "        print(\"Test set accuracy: \", accuracy.eval({X_input:X_test, y_target: y_test, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ćwiczenie 3.4\n",
    "Wytrenuj i oceń model na zbiorze danych z poprzedniego notebooka.\n",
    "Eksperymentuj z parametrami sieci tak by uzyskać jak najlepszy wynik na zbiorze testowym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(X_train, y_train, X_test, y_test,\n",
    "          hidden_layer_1_neurons=10,\n",
    "          hidden_layer_2_neurons=5,\n",
    "          hidden_layer_3_neurons=6,\n",
    "          learning_rate=0.07,\n",
    "          activation='relu',\n",
    "          n_epochs=1400,\n",
    "          l2_regularization=True,\n",
    "          l2_lambda=0.01,\n",
    "          dropout_keep_prob=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MNIST data set\n",
    "***\n",
    "Czas na zaaplikowanie stworzonego przez Ciebie modelu\n",
    "do słynnego w świecie uczenia maszynowego zbioru MNIST.\n",
    "Zawiera on obrazki pisma ręcznego o wymiarach 28 x 28 pixeli. 50000 przykładów w zbiorze treninigowym i \n",
    "10000 w zbiorze testowym. Jest to popularny zbiór do testowania rozwiązań z zakresu rozpoznawania obrazów.\n",
    "Twoim zdaniem będzie jak najlepsze dobranie parametrów sieci. Nie bój się eksperymentować!\n",
    "Uwaga - zależnie od komputera, czas nauki sieci może potrwać kilka/kilkanaście minut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_reshape = lambda x: x.reshape(-1, x.shape[1] * x.shape[2])\n",
    "x_train = mnist_reshape(x_train)\n",
    "y_train = y_train.reshape(-1,1)\n",
    "x_test = mnist_reshape(x_test)\n",
    "y_test = y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modelowanie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x_train, y_train, x_test, y_test,\n",
    "          hidden_layer_1_neurons=40,\n",
    "          hidden_layer_2_neurons=15,\n",
    "          hidden_layer_3_neurons=15,\n",
    "          learning_rate=0.01,\n",
    "          activation='relu',\n",
    "          n_epochs=200,\n",
    "          l2_regularization=False,\n",
    "          l2_lambda=0.01,\n",
    "          dropout_keep_prob=0.6, \n",
    "          optimizer='Adam')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
